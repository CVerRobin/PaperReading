{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Re_ID_TripletCrossEntropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VUDRYkDXzh6",
        "colab_type": "code",
        "outputId": "f9bffbda-8410-4c06-dbe4-0baa5d8d4cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpUX6VQvBgS7",
        "colab_type": "code",
        "outputId": "3e5b6fd7-d3fe-43f2-ab15-9d40ee3e333d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "!pip install pretrainedmodels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.4.1+cu100)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.28.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.3.0+cu100)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.17.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->pretrainedmodels) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEPAPb_ZBtLX",
        "colab_type": "text"
      },
      "source": [
        "# Prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAfslvv_Bpil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "# You only need to change this line to your dataset download path\n",
        "download_path = '/content/drive/My Drive/Re-ID/Market'\n",
        "\n",
        "if not os.path.isdir(download_path):\n",
        "    print('please change the download_path')\n",
        "\n",
        "save_path = download_path + '/pytorch'\n",
        "if not os.path.isdir(save_path):\n",
        "    os.mkdir(save_path)\n",
        "#-----------------------------------------\n",
        "#query\n",
        "query_path = download_path + '/query'\n",
        "query_save_path = download_path + '/pytorch/query'\n",
        "if not os.path.isdir(query_save_path):\n",
        "    os.mkdir(query_save_path)\n",
        "\n",
        "for root, dirs, files in os.walk(query_path, topdown=True):\n",
        "    for name in files:\n",
        "        if not name[-3:]=='jpg':\n",
        "            continue\n",
        "        ID  = name.split('_')\n",
        "        src_path = query_path + '/' + name\n",
        "        dst_path = query_save_path + '/' + ID[0] \n",
        "        if not os.path.isdir(dst_path):\n",
        "            os.mkdir(dst_path)\n",
        "        copyfile(src_path, dst_path + '/' + name)\n",
        "\n",
        "#-----------------------------------------\n",
        "#multi-query\n",
        "query_path = download_path + '/gt_bbox'\n",
        "# for dukemtmc-reid, we do not need multi-query\n",
        "if os.path.isdir(query_path):\n",
        "    query_save_path = download_path + '/pytorch/multi-query'\n",
        "    if not os.path.isdir(query_save_path):\n",
        "        os.mkdir(query_save_path)\n",
        "\n",
        "    for root, dirs, files in os.walk(query_path, topdown=True):\n",
        "        for name in files:\n",
        "            if not name[-3:]=='jpg':\n",
        "                continue\n",
        "            ID  = name.split('_')\n",
        "            src_path = query_path + '/' + name\n",
        "            dst_path = query_save_path + '/' + ID[0]\n",
        "            if not os.path.isdir(dst_path):\n",
        "                os.mkdir(dst_path)\n",
        "            copyfile(src_path, dst_path + '/' + name)\n",
        "\n",
        "#-----------------------------------------\n",
        "#gallery\n",
        "gallery_path = download_path + '/bounding_box_test'\n",
        "gallery_save_path = download_path + '/pytorch/gallery'\n",
        "if not os.path.isdir(gallery_save_path):\n",
        "    os.mkdir(gallery_save_path)\n",
        "\n",
        "for root, dirs, files in os.walk(gallery_path, topdown=True):\n",
        "    for name in files:\n",
        "        if not name[-3:]=='jpg':\n",
        "            continue\n",
        "        ID  = name.split('_')\n",
        "        src_path = gallery_path + '/' + name\n",
        "        dst_path = gallery_save_path + '/' + ID[0]\n",
        "        if not os.path.isdir(dst_path):\n",
        "            os.mkdir(dst_path)\n",
        "        copyfile(src_path, dst_path + '/' + name)\n",
        "\n",
        "#---------------------------------------\n",
        "#train_all\n",
        "train_path = download_path + '/bounding_box_train'\n",
        "train_save_path = download_path + '/pytorch/train_all'\n",
        "if not os.path.isdir(train_save_path):\n",
        "    os.mkdir(train_save_path)\n",
        "\n",
        "for root, dirs, files in os.walk(train_path, topdown=True):\n",
        "    for name in files:\n",
        "        if not name[-3:]=='jpg':\n",
        "            continue\n",
        "        ID  = name.split('_')\n",
        "        src_path = train_path + '/' + name\n",
        "        dst_path = train_save_path + '/' + ID[0]\n",
        "        if not os.path.isdir(dst_path):\n",
        "            os.mkdir(dst_path)\n",
        "        copyfile(src_path, dst_path + '/' + name)\n",
        "\n",
        "\n",
        "#---------------------------------------\n",
        "#train_val\n",
        "train_path = download_path + '/bounding_box_train'\n",
        "train_save_path = download_path + '/pytorch/train'\n",
        "val_save_path = download_path + '/pytorch/val'\n",
        "if not os.path.isdir(train_save_path):\n",
        "    os.mkdir(train_save_path)\n",
        "    os.mkdir(val_save_path)\n",
        "\n",
        "for root, dirs, files in os.walk(train_path, topdown=True):\n",
        "    for name in files:\n",
        "        if not name[-3:]=='jpg':\n",
        "            continue\n",
        "        ID  = name.split('_')\n",
        "        src_path = train_path + '/' + name\n",
        "        dst_path = train_save_path + '/' + ID[0]\n",
        "        if not os.path.isdir(dst_path):\n",
        "            os.mkdir(dst_path)\n",
        "            dst_path = val_save_path + '/' + ID[0]  #first image is used as val image\n",
        "            os.mkdir(dst_path)\n",
        "        copyfile(src_path, dst_path + '/' + name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2cwZ47xJ51e",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# random_erasing.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuDMBrD1J8ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "\n",
        "from torchvision.transforms import *\n",
        "\n",
        "#from PIL import Image\n",
        "import random\n",
        "import math\n",
        "#import numpy as np\n",
        "#import torch\n",
        "\n",
        "class RandomErasing(object):\n",
        "    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\n",
        "        'Random Erasing Data Augmentation' by Zhong et al.\n",
        "        See https://arxiv.org/pdf/1708.04896.pdf\n",
        "    Args:\n",
        "         probability: The probability that the Random Erasing operation will be performed.\n",
        "         sl: Minimum proportion of erased area against input image.\n",
        "         sh: Maximum proportion of erased area against input image.\n",
        "         r1: Minimum aspect ratio of erased area.\n",
        "         mean: Erasing value. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
        "        self.probability = probability\n",
        "        self.mean = mean\n",
        "        self.sl = sl\n",
        "        self.sh = sh\n",
        "        self.r1 = r1\n",
        "       \n",
        "    def __call__(self, img):\n",
        "\n",
        "        if random.uniform(0, 1) > self.probability:\n",
        "            return img\n",
        "\n",
        "        for attempt in range(100):\n",
        "            area = img.size()[1] * img.size()[2]\n",
        "       \n",
        "            target_area = random.uniform(self.sl, self.sh) * area\n",
        "            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n",
        "\n",
        "            h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
        "            w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "            if w < img.size()[2] and h < img.size()[1]:\n",
        "                x1 = random.randint(0, img.size()[1] - h)\n",
        "                y1 = random.randint(0, img.size()[2] - w)\n",
        "                if img.size()[0] == 3:\n",
        "                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n",
        "                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n",
        "                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n",
        "                else:\n",
        "                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n",
        "                return img\n",
        "\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ShvAk_xdxy_",
        "colab_type": "text"
      },
      "source": [
        "# TripletLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_CsKU_fdvCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class TripletSemihardLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Shape:\n",
        "        - Input: :math:`(N, C)` where `C = number of channels`\n",
        "        - Target: :math:`(N)`\n",
        "        - Output: scalar.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, margin=0, size_average=True):\n",
        "        super(TripletSemihardLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.size_average = size_average\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        y_true = target.int().unsqueeze(-1)\n",
        "        same_id = torch.eq(y_true, y_true.t()).type_as(input)\n",
        "\n",
        "        pos_mask = same_id\n",
        "        neg_mask = 1 - same_id\n",
        "\n",
        "        def _mask_max(input_tensor, mask, axis=None, keepdims=False):\n",
        "            input_tensor = input_tensor - 1e6 * (1 - mask)\n",
        "            _max, _idx = torch.max(input_tensor, dim=axis, keepdim=keepdims)\n",
        "            return _max, _idx\n",
        "\n",
        "        def _mask_min(input_tensor, mask, axis=None, keepdims=False):\n",
        "            input_tensor = input_tensor + 1e6 * (1 - mask)\n",
        "            _min, _idx = torch.min(input_tensor, dim=axis, keepdim=keepdims)\n",
        "            return _min, _idx\n",
        "\n",
        "        # output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
        "        dist_squared = torch.sum(input ** 2, dim=1, keepdim=True) + \\\n",
        "                       torch.sum(input.t() ** 2, dim=0, keepdim=True) - \\\n",
        "                       2.0 * torch.matmul(input, input.t())\n",
        "        dist = dist_squared.clamp(min=1e-16).sqrt()\n",
        "\n",
        "        pos_max, pos_idx = _mask_max(dist, pos_mask, axis=-1)\n",
        "        neg_min, neg_idx = _mask_min(dist, neg_mask, axis=-1)\n",
        "\n",
        "        # loss(x, y) = max(0, -y * (x1 - x2) + margin)\n",
        "        y = torch.ones(same_id.size()[0]).to(self.device)\n",
        "        return F.margin_ranking_loss(neg_min.float(),\n",
        "                                     pos_max.float(),\n",
        "                                     y,\n",
        "                                     self.margin,\n",
        "                                     self.size_average)\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"Triplet loss with hard positive/negative mining.\n",
        "    Reference:\n",
        "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
        "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
        "    Args:\n",
        "        margin (float): margin for triplet.\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=0.3, mutual_flag = False):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
        "        self.mutual = mutual_flag\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
        "            targets: ground truth labels with shape (num_classes)\n",
        "        \"\"\"\n",
        "        n = inputs.size(0)\n",
        "        #inputs = 1. * inputs / (torch.norm(inputs, 2, dim=-1, keepdim=True).expand_as(inputs) + 1e-12)\n",
        "        # Compute pairwise distance, replace by the official when merged\n",
        "        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
        "        dist = dist + dist.t()\n",
        "        dist.addmm_(1, -2, inputs, inputs.t())\n",
        "        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
        "        # For each anchor, find the hardest positive and negative\n",
        "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
        "        dist_ap, dist_an = [], []\n",
        "        for i in range(n):\n",
        "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
        "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
        "        dist_ap = torch.cat(dist_ap)\n",
        "        dist_an = torch.cat(dist_an)\n",
        "        # Compute ranking hinge loss\n",
        "        y = torch.ones_like(dist_an)\n",
        "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
        "        if self.mutual:\n",
        "            return loss, dist\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b58OaLwxDNMv",
        "colab_type": "text"
      },
      "source": [
        "# model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8VSA87oB36J",
        "colab_type": "code",
        "outputId": "f1e6a458-ca72-448c-a69d-7524b7cac083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torchvision import models\n",
        "from torch.autograd import Variable\n",
        "import pretrainedmodels\n",
        "import numpy as np\n",
        "\n",
        "######################################################################\n",
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    # print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in') # For old pytorch, you may use kaiming_normal.\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find('BatchNorm1d') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "def weights_init_classifier(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        init.normal_(m.weight.data, std=0.001)\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "# Defines the new fc layer and classification layer\n",
        "# |--Linear--|--bn--|--relu--|--Linear--|\n",
        "class ClassBlock(nn.Module):\n",
        "    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True, return_f = False):\n",
        "        super(ClassBlock, self).__init__()\n",
        "        self.return_f = return_f\n",
        "        add_block = []\n",
        "        if linear:\n",
        "            add_block += [nn.Linear(input_dim, num_bottleneck)]\n",
        "        else:\n",
        "            num_bottleneck = input_dim\n",
        "        if bnorm:\n",
        "            add_block += [nn.BatchNorm1d(num_bottleneck)]\n",
        "        if relu:\n",
        "            add_block += [nn.LeakyReLU(0.1)]\n",
        "        if droprate>0:\n",
        "            add_block += [nn.Dropout(p=droprate)]\n",
        "        add_block = nn.Sequential(*add_block)\n",
        "        add_block.apply(weights_init_kaiming)\n",
        "\n",
        "        classifier = []\n",
        "        classifier += [nn.Linear(num_bottleneck, class_num)]\n",
        "        classifier = nn.Sequential(*classifier)\n",
        "        classifier.apply(weights_init_classifier)\n",
        "\n",
        "        self.add_block = add_block\n",
        "        self.classifier = classifier\n",
        "    def forward(self, x):\n",
        "        x = self.add_block(x)\n",
        "        if self.return_f:\n",
        "            f = x\n",
        "            x = self.classifier(x)\n",
        "            return x,f\n",
        "        else:\n",
        "            x = self.classifier(x)\n",
        "            return x\n",
        "\n",
        "# Define the ResNet50-based Model\n",
        "class ft_net(nn.Module):\n",
        "\n",
        "    def __init__(self, class_num, droprate=0.5, stride=2):\n",
        "        super(ft_net, self).__init__()\n",
        "        model_ft = models.resnet50(pretrained=True)\n",
        "        # avg pooling to global pooling\n",
        "        if stride == 1:\n",
        "            model_ft.layer4[0].downsample[0].stride = (1,1)\n",
        "            model_ft.layer4[0].conv2.stride = (1,1)\n",
        "        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.model = model_ft\n",
        "        self.classifier = ClassBlock(2048, class_num, droprate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "        x = self.model.layer1(x)\n",
        "        x = self.model.layer2(x)\n",
        "        x = self.model.layer3(x)\n",
        "        x = self.model.layer4(x)\n",
        "        x = self.model.avgpool(x)\n",
        "        x = x.view(x.size(0), x.size(1))\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Define the DenseNet121-based Model\n",
        "class ft_net_dense(nn.Module):\n",
        "\n",
        "    def __init__(self, class_num, droprate=0.5):\n",
        "        super().__init__()\n",
        "        model_ft = models.densenet121(pretrained=True)\n",
        "        model_ft.features.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        model_ft.fc = nn.Sequential()\n",
        "        self.model = model_ft\n",
        "        # For DenseNet, the feature dim is 1024 \n",
        "        self.classifier = ClassBlock(1024, class_num, droprate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.features(x)\n",
        "        x = x.view(x.size(0), x.size(1))\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Define the NAS-based Model\n",
        "class ft_net_NAS(nn.Module):\n",
        "\n",
        "    def __init__(self, class_num, droprate=0.5):\n",
        "        super().__init__()  \n",
        "        model_name = 'nasnetalarge' \n",
        "        # pip install pretrainedmodels\n",
        "        model_ft = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
        "        model_ft.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        model_ft.dropout = nn.Sequential()\n",
        "        model_ft.last_linear = nn.Sequential()\n",
        "        self.model = model_ft\n",
        "        # For DenseNet, the feature dim is 4032\n",
        "        self.classifier = ClassBlock(4032, class_num, droprate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.features(x)\n",
        "        x = self.model.avg_pool(x)\n",
        "        x = x.view(x.size(0), x.size(1))\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "# Define the ResNet50-based Model (Middle-Concat)\n",
        "# In the spirit of \"The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching.\" Yu, Qian, et al. arXiv:1711.08106 (2017).\n",
        "class ft_net_middle(nn.Module):\n",
        "\n",
        "    def __init__(self, class_num, droprate=0.5):\n",
        "        super(ft_net_middle, self).__init__()\n",
        "        model_ft = models.resnet50(pretrained=True)\n",
        "        # avg pooling to global pooling\n",
        "        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.model = model_ft\n",
        "        self.classifier = ClassBlock(2048+1024, class_num, droprate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "        x = self.model.layer1(x)\n",
        "        x = self.model.layer2(x)\n",
        "        x = self.model.layer3(x)\n",
        "        # x0  n*1024*1*1\n",
        "        x0 = self.model.avgpool(x)\n",
        "        x = self.model.layer4(x)\n",
        "        # x1  n*2048*1*1\n",
        "        x1 = self.model.avgpool(x)\n",
        "        x = torch.cat((x0,x1),1)\n",
        "        x = x.view(x.size(0), x.size(1))\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Part Model proposed in Yifan Sun etal. (2018)\n",
        "class PCB(nn.Module):\n",
        "    def __init__(self, class_num ):\n",
        "        super(PCB, self).__init__()\n",
        "\n",
        "        self.part = 6 # We cut the pool5 to 6 parts\n",
        "        model_ft = models.resnet50(pretrained=True)\n",
        "        self.model = model_ft\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((self.part,1))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        # remove the final downsample\n",
        "        self.model.layer4[0].downsample[0].stride = (1,1)\n",
        "        self.model.layer4[0].conv2.stride = (1,1)\n",
        "        # define 6 classifiers\n",
        "        for i in range(self.part):\n",
        "            name = 'classifier'+str(i)\n",
        "            setattr(self, name, ClassBlock(2048, class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "        \n",
        "        x = self.model.layer1(x)\n",
        "        x = self.model.layer2(x)\n",
        "        x = self.model.layer3(x)\n",
        "        x = self.model.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.dropout(x)\n",
        "        part = {}\n",
        "        predict = {}\n",
        "        # get six part feature batchsize*2048*6\n",
        "        for i in range(self.part):\n",
        "            part[i] = torch.squeeze(x[:,:,i])\n",
        "            name = 'classifier'+str(i)\n",
        "            c = getattr(self,name)\n",
        "            predict[i] = c(part[i])\n",
        "\n",
        "        # sum prediction\n",
        "        #y = predict[0]\n",
        "        #for i in range(self.part-1):\n",
        "        #    y += predict[i+1]\n",
        "        y = []\n",
        "        for i in range(self.part):\n",
        "            y.append(predict[i])\n",
        "        return y\n",
        "\n",
        "class PCB_test(nn.Module):\n",
        "    def __init__(self,model):\n",
        "        super(PCB_test,self).__init__()\n",
        "        self.part = 6\n",
        "        self.model = model.model\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((self.part,1))\n",
        "        # remove the final downsample\n",
        "        self.model.layer4[0].downsample[0].stride = (1,1)\n",
        "        self.model.layer4[0].conv2.stride = (1,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "\n",
        "        x = self.model.layer1(x)\n",
        "        x = self.model.layer2(x)\n",
        "        x = self.model.layer3(x)\n",
        "        x = self.model.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        y = x.view(x.size(0),x.size(1),x.size(2))\n",
        "        return y\n",
        "\n",
        "class HPM_PCB(nn.Module):\n",
        "    def __init__(self, class_num, parts):\n",
        "        super(HPM_PCB, self).__init__()\n",
        "\n",
        "        self.parts = parts # We cut the pool5 to 6 parts\n",
        "        model_ft = models.resnet50(pretrained=True)\n",
        "        self.model = model_ft\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        # remove the final downsample\n",
        "        self.model.layer4[0].downsample[0].stride = (1,1)\n",
        "        self.model.layer4[0].conv2.stride = (1,1)\n",
        "        idx = np.sum(self.parts)\n",
        "        # define i*j classifiers\n",
        "        self.avgpools = []\n",
        "        for i in range(len(self.parts)):\n",
        "            self.avgpools.append(nn.AdaptiveAvgPool2d((self.parts[i],1)))\n",
        "        for i in range(idx):\n",
        "            name = 'classifier' + str(i)\n",
        "            setattr(self, name, ClassBlock(2048, class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "        \n",
        "        x = self.model.layer1(x)\n",
        "        x = self.model.layer2(x)\n",
        "        x = self.model.layer3(x)\n",
        "        x = self.model.layer4(x)\n",
        "        temp = self.avgpools[0](x)\n",
        "        for i in range(1, len(self.avgpools)):\n",
        "            temp = torch.cat((temp, self.avgpools[i](x)),2)\n",
        "        x = temp\n",
        "        x = self.dropout(x)\n",
        "        part = {}\n",
        "        predict = {}\n",
        "        # get six part feature batchsize*2048*6\n",
        "        idx = np.sum(self.parts)\n",
        "        for i in range(idx):\n",
        "            part[i] = torch.squeeze(x[:,:,i])\n",
        "            name = 'classifier' + str(i)\n",
        "            c = getattr(self,name)\n",
        "            predict[i] = c(part[i])\n",
        "\n",
        "        # sum prediction\n",
        "        #y = predict[0]\n",
        "        #for i in range(self.part-1):\n",
        "        #    y += predict[i+1]\n",
        "        y = []\n",
        "        for i in range(idx):\n",
        "            y.append(predict[i])\n",
        "        return y  \n",
        "    \n",
        "class HPM_PCB_test(nn.Module):\n",
        "    def __init__(self,model,parts):\n",
        "        super(HPM_PCB_test,self).__init__()\n",
        "        self.parts = parts\n",
        "        self.model = model.model\n",
        "        self.avgpools = []\n",
        "        for i in range(len(self.parts)):\n",
        "            self.avgpools.append(nn.AdaptiveAvgPool2d((self.parts[i],1)))\n",
        "        # remove the final downsample\n",
        "        self.model.layer4[0].downsample[0].stride = (1,1)\n",
        "        self.model.layer4[0].conv2.stride = (1,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "\n",
        "        x = self.model.layer1(x)\n",
        "        x = self.model.layer2(x)\n",
        "        x = self.model.layer3(x)\n",
        "        x = self.model.layer4(x)\n",
        "        temp = self.avgpools[0](x)\n",
        "        for i in range(1, len(self.avgpools)):\n",
        "            temp = torch.cat((temp, self.avgpools[i](x)),2)\n",
        "        x = temp\n",
        "        y = x.view(x.size(0),x.size(1),x.size(2))\n",
        "        return y\n",
        "\n",
        "'''\n",
        "# debug model structure\n",
        "# Run this code with:\n",
        "python model.py\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "# Here I left a simple forward function.\n",
        "# Test the model, before you train it. \n",
        "    net = ft_net(751, stride=1)\n",
        "    net.classifier = nn.Sequential()\n",
        "    print(net)\n",
        "    input = Variable(torch.FloatTensor(8, 3, 256, 128))\n",
        "    output = net(input)\n",
        "    print('net output size:')\n",
        "    print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ft_net(\n",
            "  (model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            "  )\n",
            "  (classifier): Sequential()\n",
            ")\n",
            "net output size:\n",
            "torch.Size([8, 2048])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI-W3PoPDb3C",
        "colab_type": "text"
      },
      "source": [
        "# train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vFvw7sHDc-X",
        "colab_type": "code",
        "outputId": "4e61749f-61ac-4428-c180-09cd6ae85198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "#from PIL import Image\n",
        "import time\n",
        "import os\n",
        "# from model import ft_net, ft_net_dense, ft_net_NAS, PCB\n",
        "# from random_erasing import RandomErasing\n",
        "import yaml\n",
        "import math\n",
        "from shutil import copyfile\n",
        "\n",
        "version =  torch.__version__\n",
        "#fp16\n",
        "try:\n",
        "    from apex.fp16_utils import *\n",
        "    from apex import amp, optimizers\n",
        "except ImportError: # will be 3.x series\n",
        "    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n",
        "######################################################################\n",
        "# Options\n",
        "# --------\n",
        "opt = {}\n",
        "opt['gpu_ids'] = '0'\n",
        "opt['name'] = 'ft_ResNet50'\n",
        "opt['data_dir'] = '/content/drive/My Drive/Re-ID/Market/pytorch'\n",
        "opt['train_all'] = 'store_true'\n",
        "opt['color_jitter'] = 'store_true'\n",
        "opt['batchsize'] = 32\n",
        "opt['stride'] = 2\n",
        "opt['erasing_p'] = 0\n",
        "opt['use_dense'] = 'store_true'\n",
        "opt['use_NAS'] = 'store_true'\n",
        "opt['warm_epoch'] = 0\n",
        "opt['lr'] = 0.05\n",
        "opt['droprate'] = 0.5\n",
        "opt['PCB'] = 'store_true'\n",
        "opt['fp16'] = 'store_true'\n",
        "\n",
        "fp16 = opt['fp16']\n",
        "data_dir = opt['data_dir']\n",
        "name = opt['name']\n",
        "str_ids = opt['gpu_ids'].split(',')\n",
        "gpu_ids = []\n",
        "for str_id in str_ids:\n",
        "    gid = int(str_id)\n",
        "    if gid >=0:\n",
        "        gpu_ids.append(gid)\n",
        "        \n",
        "use_gpu = torch.cuda.is_available()\n",
        "# set gpu ids\n",
        "if use_gpu and len(gpu_ids)>0:\n",
        "    torch.cuda.set_device(gpu_ids[0])\n",
        "    cudnn.benchmark = True\n",
        "######################################################################\n",
        "# Load Data\n",
        "# ---------\n",
        "#\n",
        "\n",
        "transform_train_list = [\n",
        "        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n",
        "        transforms.Resize((256,128), interpolation=3),\n",
        "        transforms.Pad(10),\n",
        "        transforms.RandomCrop((256,128)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]\n",
        "\n",
        "transform_val_list = [\n",
        "        transforms.Resize(size=(256,128),interpolation=3), #Image.BICUBIC\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]\n",
        "\n",
        "if opt['PCB']:\n",
        "    transform_train_list = [\n",
        "        transforms.Resize((384,192), interpolation=3),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]\n",
        "    transform_val_list = [\n",
        "        transforms.Resize(size=(384,192),interpolation=3), #Image.BICUBIC\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]\n",
        "\n",
        "if opt['erasing_p']>0:\n",
        "    transform_train_list = transform_train_list +  [RandomErasing(probability = opt['erasing_p'], mean=[0.0, 0.0, 0.0])]\n",
        "\n",
        "if opt['color_jitter']:\n",
        "    transform_train_list = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0)] + transform_train_list\n",
        "\n",
        "print(transform_train_list)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose( transform_train_list ),\n",
        "    'val': transforms.Compose(transform_val_list),\n",
        "}\n",
        "\n",
        "\n",
        "train_all = ''\n",
        "if opt['train_all']:\n",
        "     train_all = '_all'\n",
        "\n",
        "image_datasets = {}\n",
        "image_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train' + train_all),\n",
        "                                          data_transforms['train'])\n",
        "image_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n",
        "                                          data_transforms['val'])\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt['batchsize'],\n",
        "                                             shuffle=True, num_workers=8, pin_memory=True) # 8 workers may work faster\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "since = time.time()\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "print(time.time()-since)\n",
        "######################################################################\n",
        "# Training the model\n",
        "# ------------------\n",
        "#\n",
        "# Now, let's write a general function to train a model. Here, we will\n",
        "# illustrate:\n",
        "#\n",
        "# -  Scheduling the learning rate\n",
        "# -  Saving the best model\n",
        "#\n",
        "# In the following, parameter ``scheduler`` is an LR scheduler object from\n",
        "# ``torch.optim.lr_scheduler``.\n",
        "\n",
        "y_loss = {} # loss history\n",
        "y_loss['train'] = []\n",
        "y_loss['val'] = []\n",
        "y_err = {}\n",
        "y_err['train'] = []\n",
        "y_err['val'] = []\n",
        "\n",
        "def train_model(model, criterions, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    #best_model_wts = model.state_dict()\n",
        "    #best_acc = 0.0\n",
        "    warm_up = 0.1 # We start from the 0.1*lrRate\n",
        "    warm_iteration = round(dataset_sizes['train']/opt['batchsize'])*opt['warm_epoch'] # first 5 epoch\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train(True)  # Set model to training mode\n",
        "            else:\n",
        "                model.train(False)  # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "            # Iterate over data.\n",
        "            for data in dataloaders[phase]:\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                now_batch_size,c,h,w = inputs.shape\n",
        "                if now_batch_size<opt['batchsize']: # skip the last batch\n",
        "                    continue\n",
        "                #print(inputs.shape)\n",
        "                # wrap them in Variable\n",
        "                if use_gpu:\n",
        "                    inputs = Variable(inputs.cuda().detach())\n",
        "                    labels = Variable(labels.cuda().detach())\n",
        "                else:\n",
        "                    inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "                # if we use low precision, input also need to be fp16\n",
        "                #if fp16:\n",
        "                #    inputs = inputs.half()\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                if phase == 'val':\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(inputs)\n",
        "                else:\n",
        "                    outputs = model(inputs)\n",
        "\n",
        "                if not opt['PCB']:\n",
        "                    _, preds = torch.max(outputs.data, 1)\n",
        "                    loss = criterions[0](outputs, labels) + criterions[1](outputs, labels)\n",
        "                else:\n",
        "                    part = {}\n",
        "                    sm = nn.Softmax(dim=1)\n",
        "                    num_part = 15\n",
        "                    for i in range(num_part):\n",
        "                        part[i] = outputs[i]\n",
        "\n",
        "                    score = 0\n",
        "                    for i in range(15):\n",
        "                        score += sm(part[i])\n",
        "                    _, preds = torch.max(score.data, 1)\n",
        "\n",
        "                    loss = criterions[1](part[0], labels)\n",
        "                    for i in range(num_part-1):\n",
        "                        loss += criterions[0](part[i+1], labels)\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if epoch<opt['warm_epoch'] and phase == 'train': \n",
        "                    warm_up = min(1.0, warm_up + 0.9 / warm_iteration)\n",
        "                    loss *= warm_up\n",
        "\n",
        "                if phase == 'train':\n",
        "#                     if fp16: # we use optimier to backward loss\n",
        "#                         with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "#                             scaled_loss.backward()\n",
        "#                     else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                if int(version[0])>0 or int(version[2]) > 3: # for the new version like 0.4.0, 0.5.0 and 1.0.0\n",
        "                    running_loss += loss.item() * now_batch_size\n",
        "                else :  # for the old version like 0.3.0 and 0.3.1\n",
        "                    running_loss += loss.data[0] * now_batch_size\n",
        "                running_corrects += float(torch.sum(preds == labels.data))\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            y_loss[phase].append(epoch_loss)\n",
        "            y_err[phase].append(1.0-epoch_acc)            \n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "                last_model_wts = model.state_dict()\n",
        "                if epoch%10 == 9:\n",
        "                    save_network(model, epoch)\n",
        "                draw_curve(epoch)\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "            time_elapsed // 60, time_elapsed % 60))\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(last_model_wts)\n",
        "    save_network(model, 'last')\n",
        "    return model\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Draw Curve\n",
        "#---------------------------\n",
        "x_epoch = []\n",
        "fig = plt.figure()\n",
        "ax0 = fig.add_subplot(121, title=\"loss\")\n",
        "ax1 = fig.add_subplot(122, title=\"top1err\")\n",
        "def draw_curve(current_epoch):\n",
        "    x_epoch.append(current_epoch)\n",
        "    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='train')\n",
        "    ax0.plot(x_epoch, y_loss['val'], 'ro-', label='val')\n",
        "    ax1.plot(x_epoch, y_err['train'], 'bo-', label='train')\n",
        "    ax1.plot(x_epoch, y_err['val'], 'ro-', label='val')\n",
        "    if current_epoch == 0:\n",
        "        ax0.legend()\n",
        "        ax1.legend()\n",
        "    fig.savefig( os.path.join('/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/model',name,'HPM_triplet_crossentropy_train.jpg'))\n",
        "\n",
        "######################################################################\n",
        "# Save model\n",
        "#---------------------------\n",
        "def save_network(network, epoch_label):\n",
        "    save_filename = 'HPM_triplet_crossentropy_net_%s.pth'% epoch_label\n",
        "    save_path = os.path.join('/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/model',name,save_filename)\n",
        "    torch.save(network.cpu().state_dict(), save_path)\n",
        "    if torch.cuda.is_available():\n",
        "        network.cuda(gpu_ids[0])\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Finetuning the convnet\n",
        "# ----------------------\n",
        "#\n",
        "# Load a pretrainied model and reset final fully connected layer.\n",
        "#\n",
        "\n",
        "if opt['use_dense']:\n",
        "    model = ft_net_dense(len(class_names), opt['droprate'])\n",
        "elif opt['use_NAS']:\n",
        "    model = ft_net_NAS(len(class_names), opt['droprate'])\n",
        "else:\n",
        "    model = ft_net(len(class_names), opt['droprate'], opt['stride'])\n",
        "\n",
        "parts = [1,2,4,8]\n",
        "if opt['PCB']:\n",
        "    model = HPM_PCB(len(class_names),parts)\n",
        "\n",
        "opt['nclasses'] = len(class_names)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "if not opt['PCB']:\n",
        "    ignored_params = list(map(id, model.classifier.parameters() ))\n",
        "    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n",
        "    optimizer_ft = optim.SGD([\n",
        "             {'params': base_params, 'lr': 0.1*opt['lr']},\n",
        "             {'params': model.classifier.parameters(), 'lr': opt['lr']}\n",
        "         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\n",
        "else:\n",
        "    ignored_params = list(map(id, model.model.fc.parameters() ))\n",
        "    ignored_params += (list(map(id, model.classifier0.parameters() )) \n",
        "                     +list(map(id, model.classifier1.parameters() ))\n",
        "                     +list(map(id, model.classifier2.parameters() ))\n",
        "                     +list(map(id, model.classifier3.parameters() ))\n",
        "                     +list(map(id, model.classifier4.parameters() ))\n",
        "                     +list(map(id, model.classifier5.parameters() ))\n",
        "                     +list(map(id, model.classifier6.parameters() ))\n",
        "                     +list(map(id, model.classifier7.parameters() ))\n",
        "                     +list(map(id, model.classifier8.parameters() ))\n",
        "                     +list(map(id, model.classifier9.parameters() ))\n",
        "                     +list(map(id, model.classifier10.parameters() ))\n",
        "                     +list(map(id, model.classifier11.parameters() ))\n",
        "                     +list(map(id, model.classifier12.parameters() ))\n",
        "                     +list(map(id, model.classifier13.parameters() ))\n",
        "                     +list(map(id, model.classifier14.parameters() ))\n",
        "                      )\n",
        "    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n",
        "    optimizer_ft = optim.SGD([\n",
        "             {'params': base_params, 'lr': 0.1*opt['lr']},\n",
        "             {'params': model.model.fc.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier0.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier1.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier2.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier3.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier4.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier5.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier6.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier7.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier8.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier9.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier10.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier11.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier12.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier13.parameters(), 'lr': opt['lr']},\n",
        "             {'params': model.classifier14.parameters(), 'lr': opt['lr']},\n",
        "#              {'params': model.classifier6.parameters(), 'lr': 0.01},\n",
        "#              {'params': model.classifier7.parameters(), 'lr': 0.01}\n",
        "         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 40 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=40, gamma=0.1)\n",
        "\n",
        "######################################################################\n",
        "# Train and evaluate\n",
        "# ^^^^^^^^^^^^^^^^^^\n",
        "#\n",
        "# It should take around 1-2 hours on GPU. \n",
        "#\n",
        "dir_name = os.path.join('/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/model',name)\n",
        "\n",
        "# model to gpu\n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "\n",
        "criterions = []\n",
        "criterions.append(nn.CrossEntropyLoss())\n",
        "criterions.append(TripletLoss())\n",
        "\n",
        "model = train_model(model, criterions, optimizer_ft, exp_lr_scheduler, num_epochs=50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n",
            "[ColorJitter(brightness=[0.9, 1.1], contrast=[0.9, 1.1], saturation=[0.9, 1.1], hue=None), Resize(size=(384, 192), interpolation=PIL.Image.BICUBIC), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
            "22.385034799575806\n",
            "Epoch 0/49\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 74.8424 Acc: 0.1551\n",
            "val Loss: 58.3841 Acc: 0.2543\n",
            "Training complete in 9m 50s\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 50.3484 Acc: 0.4661\n",
            "val Loss: 43.3733 Acc: 0.4647\n",
            "Training complete in 18m 46s\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 38.8462 Acc: 0.6533\n",
            "val Loss: 31.7739 Acc: 0.6445\n",
            "Training complete in 27m 42s\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 31.0363 Acc: 0.7801\n",
            "val Loss: 23.9774 Acc: 0.7710\n",
            "Training complete in 36m 37s\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 25.7626 Acc: 0.8471\n",
            "val Loss: 19.4480 Acc: 0.8375\n",
            "Training complete in 45m 34s\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 21.3386 Acc: 0.8968\n",
            "val Loss: 20.4369 Acc: 0.8176\n",
            "Training complete in 54m 29s\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 18.3476 Acc: 0.9294\n",
            "val Loss: 12.1443 Acc: 0.9148\n",
            "Training complete in 63m 28s\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 15.7191 Acc: 0.9481\n",
            "val Loss: 10.5937 Acc: 0.9228\n",
            "Training complete in 72m 26s\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 13.5730 Acc: 0.9646\n",
            "val Loss: 7.5520 Acc: 0.9561\n",
            "Training complete in 81m 25s\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 11.7088 Acc: 0.9746\n",
            "val Loss: 7.6386 Acc: 0.9614\n",
            "Training complete in 90m 27s\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 10.2152 Acc: 0.9797\n",
            "val Loss: 5.9045 Acc: 0.9640\n",
            "Training complete in 99m 26s\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 9.1127 Acc: 0.9868\n",
            "val Loss: 4.6685 Acc: 0.9667\n",
            "Training complete in 108m 25s\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 8.1449 Acc: 0.9898\n",
            "val Loss: 4.1458 Acc: 0.9707\n",
            "Training complete in 117m 24s\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 7.1530 Acc: 0.9928\n",
            "val Loss: 3.3947 Acc: 0.9747\n",
            "Training complete in 126m 24s\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 6.2357 Acc: 0.9943\n",
            "val Loss: 2.4685 Acc: 0.9774\n",
            "Training complete in 135m 22s\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 5.5641 Acc: 0.9949\n",
            "val Loss: 2.7640 Acc: 0.9747\n",
            "Training complete in 144m 21s\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 4.9899 Acc: 0.9969\n",
            "val Loss: 1.8617 Acc: 0.9787\n",
            "Training complete in 153m 19s\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 4.4502 Acc: 0.9978\n",
            "val Loss: 1.7128 Acc: 0.9787\n",
            "Training complete in 162m 17s\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 3.7715 Acc: 0.9983\n",
            "val Loss: 2.0198 Acc: 0.9787\n",
            "Training complete in 171m 14s\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 3.3252 Acc: 0.9990\n",
            "val Loss: 1.1013 Acc: 0.9800\n",
            "Training complete in 180m 14s\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 2.8304 Acc: 0.9992\n",
            "val Loss: 0.9672 Acc: 0.9800\n",
            "Training complete in 189m 11s\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 2.5712 Acc: 0.9990\n",
            "val Loss: 0.7725 Acc: 0.9800\n",
            "Training complete in 198m 8s\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 2.1900 Acc: 0.9992\n",
            "val Loss: 0.6371 Acc: 0.9800\n",
            "Training complete in 207m 4s\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 1.9171 Acc: 0.9991\n",
            "val Loss: 0.4148 Acc: 0.9800\n",
            "Training complete in 216m 0s\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 1.5995 Acc: 0.9993\n",
            "val Loss: 0.3667 Acc: 0.9800\n",
            "Training complete in 224m 57s\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 1.4195 Acc: 0.9994\n",
            "val Loss: 0.2928 Acc: 0.9800\n",
            "Training complete in 233m 54s\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 1.1796 Acc: 0.9994\n",
            "val Loss: 0.2378 Acc: 0.9800\n",
            "Training complete in 242m 50s\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 1.0147 Acc: 0.9994\n",
            "val Loss: 0.1819 Acc: 0.9800\n",
            "Training complete in 251m 47s\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.8621 Acc: 0.9994\n",
            "val Loss: 0.1826 Acc: 0.9800\n",
            "Training complete in 260m 43s\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.7848 Acc: 0.9994\n",
            "val Loss: 0.1396 Acc: 0.9800\n",
            "Training complete in 269m 42s\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.6954 Acc: 0.9994\n",
            "val Loss: 0.1474 Acc: 0.9800\n",
            "Training complete in 278m 40s\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.6275 Acc: 0.9994\n",
            "val Loss: 0.1310 Acc: 0.9800\n",
            "Training complete in 287m 39s\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.5898 Acc: 0.9994\n",
            "val Loss: 0.1053 Acc: 0.9800\n",
            "Training complete in 296m 36s\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.5627 Acc: 0.9994\n",
            "val Loss: 0.0925 Acc: 0.9800\n",
            "Training complete in 305m 31s\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.5475 Acc: 0.9994\n",
            "val Loss: 0.1066 Acc: 0.9800\n",
            "Training complete in 314m 30s\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.5372 Acc: 0.9994\n",
            "val Loss: 0.0995 Acc: 0.9800\n",
            "Training complete in 323m 27s\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.5276 Acc: 0.9994\n",
            "val Loss: 0.0906 Acc: 0.9800\n",
            "Training complete in 332m 26s\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.5008 Acc: 0.9994\n",
            "val Loss: 0.0993 Acc: 0.9800\n",
            "Training complete in 341m 24s\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.5244 Acc: 0.9994\n",
            "val Loss: 0.0930 Acc: 0.9800\n",
            "Training complete in 350m 22s\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.4323 Acc: 0.9994\n",
            "val Loss: 0.0666 Acc: 0.9800\n",
            "Training complete in 359m 21s\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.3846 Acc: 0.9994\n",
            "val Loss: 0.0674 Acc: 0.9800\n",
            "Training complete in 368m 18s\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.3690 Acc: 0.9994\n",
            "val Loss: 0.0634 Acc: 0.9800\n",
            "Training complete in 377m 12s\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.3658 Acc: 0.9994\n",
            "val Loss: 0.0612 Acc: 0.9800\n",
            "Training complete in 386m 6s\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.3641 Acc: 0.9994\n",
            "val Loss: 0.0611 Acc: 0.9800\n",
            "Training complete in 395m 4s\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.3606 Acc: 0.9994\n",
            "val Loss: 0.0620 Acc: 0.9800\n",
            "Training complete in 404m 2s\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.3638 Acc: 0.9994\n",
            "val Loss: 0.0629 Acc: 0.9800\n",
            "Training complete in 413m 0s\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.3660 Acc: 0.9994\n",
            "val Loss: 0.0664 Acc: 0.9800\n",
            "Training complete in 421m 59s\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.3701 Acc: 0.9994\n",
            "val Loss: 0.0660 Acc: 0.9800\n",
            "Training complete in 430m 59s\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.3701 Acc: 0.9994\n",
            "val Loss: 0.0675 Acc: 0.9800\n",
            "Training complete in 439m 59s\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.3742 Acc: 0.9994\n",
            "val Loss: 0.0704 Acc: 0.9800\n",
            "Training complete in 448m 58s\n",
            "\n",
            "Training complete in 448m 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hS05QLQuSIZ",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiIXzwDxR5RH",
        "colab_type": "code",
        "outputId": "cc4bc0db-b7b0-48c2-c0a5-c818a9b8c6ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import scipy.io\n",
        "import yaml\n",
        "import math\n",
        "# from model import ft_net, ft_net_dense, ft_net_NAS, PCB, PCB_test\n",
        "\n",
        "#fp16\n",
        "try:\n",
        "    from apex.fp16_utils import *\n",
        "except ImportError: # will be 3.x series\n",
        "    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n",
        "######################################################################\n",
        "# Options\n",
        "# --------\n",
        "\n",
        "opt = {}\n",
        "opt['gpu_ids'] = '0'\n",
        "opt['which_epoch'] = 'last'\n",
        "opt['test_dir'] = '/content/drive/My Drive/Re-ID/Market/pytorch'\n",
        "opt['name'] = 'ft_ResNet50'\n",
        "opt['batchsize'] = 32\n",
        "opt['use_dense'] = 'store_true'\n",
        "opt['PCB'] = 'store_true'\n",
        "opt['multi'] = 'store_true'\n",
        "opt['fp16'] = 'store_true'\n",
        "opt['ms'] = '1'\n",
        "\n",
        "\n",
        "###load config###\n",
        "# load the training config\n",
        "# config_path = os.path.join('/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/model',opt['name'],'opts.yaml')\n",
        "# with open(config_path, 'r') as stream:\n",
        "#         config = yaml.load(stream)\n",
        "# opt['fp16']  = config['fp16'] \n",
        "# opt['PCB'] = config['PCB']\n",
        "# opt['use_dense'] = config['use_dense']\n",
        "# opt['use_NAS' = config['use_NAS']\n",
        "# opt['stride'] = config['stride']\n",
        "\n",
        "# if 'nclasses' in config: # tp compatible with old config files\n",
        "#     opt['nclasses'] = config['nclasses']\n",
        "# else: \n",
        "opt['nclasses'] = 751 \n",
        "\n",
        "str_ids = opt['gpu_ids'].split(',')\n",
        "#which_epoch = opt.which_epoch\n",
        "name = opt['name']\n",
        "test_dir = opt['test_dir']\n",
        "\n",
        "gpu_ids = []\n",
        "for str_id in str_ids:\n",
        "    id = int(str_id)\n",
        "    if id >=0:\n",
        "        gpu_ids.append(id)\n",
        "\n",
        "print('We use the scale: %s'%opt['ms'])\n",
        "str_ms = opt['ms'].split(',')\n",
        "ms = []\n",
        "for s in str_ms:\n",
        "    s_f = float(s)\n",
        "    ms.append(math.sqrt(s_f))\n",
        "\n",
        "# set gpu ids\n",
        "if len(gpu_ids)>0:\n",
        "    torch.cuda.set_device(gpu_ids[0])\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "######################################################################\n",
        "# Load Data\n",
        "# ---------\n",
        "#\n",
        "# We will use torchvision and torch.utils.data packages for loading the\n",
        "# data.\n",
        "#\n",
        "data_transforms = transforms.Compose([\n",
        "        transforms.Resize((256,128), interpolation=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "############### Ten Crop        \n",
        "        #transforms.TenCrop(224),\n",
        "        #transforms.Lambda(lambda crops: torch.stack(\n",
        "         #   [transforms.ToTensor()(crop) \n",
        "          #      for crop in crops]\n",
        "           # )),\n",
        "        #transforms.Lambda(lambda crops: torch.stack(\n",
        "         #   [transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(crop)\n",
        "          #       for crop in crops]\n",
        "          # ))\n",
        "])\n",
        "\n",
        "if opt['PCB']:\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize((384,192), interpolation=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
        "    ])\n",
        "\n",
        "\n",
        "data_dir = test_dir\n",
        "print(data_dir)\n",
        "\n",
        "# if opt['multi']:\n",
        "#     image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query','multi-query']}\n",
        "#     dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt['batchsize'],\n",
        "#                                              shuffle=False, num_workers=16) for x in ['gallery','query','multi-query']}\n",
        "# else:\n",
        "image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt['batchsize'],\n",
        "                                             shuffle=False, num_workers=16) for x in ['gallery','query']}\n",
        "class_names = image_datasets['query'].classes\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "######################################################################\n",
        "# Load model\n",
        "#---------------------------\n",
        "def load_network(network):\n",
        "    save_path = os.path.join('/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/model',name,'HPM_triplet_crossentropy_net_%s.pth'%opt['which_epoch'])\n",
        "    network.load_state_dict(torch.load(save_path))\n",
        "    print('Model already loaded!')\n",
        "    return network\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Extract feature\n",
        "# ----------------------\n",
        "#\n",
        "# Extract feature from  a trained model.\n",
        "#\n",
        "def fliplr(img):\n",
        "    '''flip horizontal'''\n",
        "    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n",
        "    img_flip = img.index_select(3,inv_idx)\n",
        "    return img_flip\n",
        "\n",
        "def extract_feature(model,dataloaders):\n",
        "    features = torch.FloatTensor()\n",
        "    count = 0\n",
        "    for data in dataloaders:\n",
        "        img, label = data\n",
        "        n, c, h, w = img.size()\n",
        "        count += n\n",
        "        print(count)\n",
        "        ff = torch.FloatTensor(n,512).zero_().cuda()\n",
        "        if opt['PCB']:\n",
        "            ff = torch.FloatTensor(n,2048,15).zero_().cuda() # we have six parts\n",
        "\n",
        "        for i in range(2):\n",
        "            if(i==1):\n",
        "                img = fliplr(img)\n",
        "            input_img = Variable(img.cuda())\n",
        "            for scale in ms:\n",
        "                if scale != 1:\n",
        "                    # bicubic is only  available in pytorch>= 1.1\n",
        "                    input_img = nn.functional.interpolate(input_img, scale_factor=scale, mode='bicubic', align_corners=False)\n",
        "                outputs = model(input_img)\n",
        "                ff += outputs\n",
        "        # norm feature\n",
        "        if opt['PCB']:\n",
        "            # feature size (n,2048,6)\n",
        "            # 1. To treat every part equally, I calculate the norm for every 2048-dim part feature.\n",
        "            # 2. To keep the cosine score==1, sqrt(6) is added to norm the whole feature (2048*6).\n",
        "            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True) * np.sqrt(15) \n",
        "            ff = ff.div(fnorm.expand_as(ff))\n",
        "            ff = ff.view(ff.size(0), -1)\n",
        "        else:\n",
        "            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
        "            ff = ff.div(fnorm.expand_as(ff))\n",
        "\n",
        "        features = torch.cat((features,ff.data.cpu()), 0)\n",
        "    return features\n",
        "\n",
        "def get_id(img_path):\n",
        "    camera_id = []\n",
        "    labels = []\n",
        "    for path, v in img_path:\n",
        "        #filename = path.split('/')[-1]\n",
        "        filename = os.path.basename(path)\n",
        "        label = filename[0:4]\n",
        "        camera = filename.split('c')[1]\n",
        "        if label[0:2]=='-1':\n",
        "            labels.append(-1)\n",
        "        else:\n",
        "            labels.append(int(label))\n",
        "        camera_id.append(int(camera[0]))\n",
        "    return camera_id, labels\n",
        "\n",
        "gallery_path = image_datasets['gallery'].imgs\n",
        "query_path = image_datasets['query'].imgs\n",
        "\n",
        "gallery_cam,gallery_label = get_id(gallery_path)\n",
        "query_cam,query_label = get_id(query_path)\n",
        "\n",
        "# if opt['multi']:\n",
        "#     mquery_path = image_datasets['multi-query'].imgs\n",
        "#     mquery_cam,mquery_label = get_id(mquery_path)\n",
        "\n",
        "######################################################################\n",
        "# Load Collected data Trained model\n",
        "print('-------test-----------')\n",
        "if opt['use_dense']:\n",
        "    model_structure = ft_net_dense(opt['nclasses'])\n",
        "elif opt['use_NAS']:\n",
        "    model_structure = ft_net_NAS(opt['nclasses'])\n",
        "else:\n",
        "    model_structure = ft_net(opt['nclasses'], stride = opt['stride'])\n",
        "parts = [1,2,4,8]\n",
        "if opt['PCB']:\n",
        "    model_structure = HPM_PCB(opt['nclasses'], parts)\n",
        "\n",
        "#if opt.fp16:\n",
        "#    model_structure = network_to_half(model_structure)\n",
        "\n",
        "model = load_network(model_structure)\n",
        "\n",
        "\n",
        "# Remove the final fc layer and classifier layer\n",
        "if opt['PCB']:\n",
        "    #if opt.fp16:\n",
        "    #    model = PCB_test(model[1])\n",
        "    #else:\n",
        "    model = HPM_PCB_test(model, parts)\n",
        "else:\n",
        "    #if opt.fp16:\n",
        "        #model[1].model.fc = nn.Sequential()\n",
        "        #model[1].classifier = nn.Sequential()\n",
        "    #else:\n",
        "    model.classifier.classifier = nn.Sequential()\n",
        "\n",
        "# Change to test mode\n",
        "model = model.eval()\n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "\n",
        "# Extract feature\n",
        "with torch.no_grad():\n",
        "    gallery_feature = extract_feature(model,dataloaders['gallery'])\n",
        "    query_feature = extract_feature(model,dataloaders['query'])\n",
        "    # if opt['multi']:\n",
        "    #     mquery_feature = extract_feature(model,dataloaders['multi-query'])\n",
        "    \n",
        "# Save to Matlab for check\n",
        "result = {'gallery_f':gallery_feature.numpy(),'gallery_label':gallery_label,'gallery_cam':gallery_cam,'query_f':query_feature.numpy(),'query_label':query_label,'query_cam':query_cam}\n",
        "scipy.io.savemat('/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/model/HPM_tripletcrossentropy_pytorch_result.mat',result)\n",
        "\n",
        "print(opt['name'])\n",
        "# result = '/content/drive/\"My Drive\"/Re-ID/Person_reID_baseline_pytorch/model/%s/HPM_result.txt'%opt['name']\n",
        "# os.system(\"python '/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/HPM_evaluate_gpu.py' | tee -a %s\"%result)\n",
        "\n",
        "# if opt['multi']:\n",
        "#     result = {'mquery_f':mquery_feature.numpy(),'mquery_label':mquery_label,'mquery_cam':mquery_cam}\n",
        "#     scipy.io.savemat('/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/model/HPM_multi_query.mat',result)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n",
            "We use the scale: 1\n",
            "/content/drive/My Drive/Re-ID/Market/pytorch\n",
            "-------test-----------\n",
            "Model already loaded!\n",
            "32\n",
            "64\n",
            "96\n",
            "128\n",
            "160\n",
            "192\n",
            "224\n",
            "256\n",
            "288\n",
            "320\n",
            "352\n",
            "384\n",
            "416\n",
            "448\n",
            "480\n",
            "512\n",
            "544\n",
            "576\n",
            "608\n",
            "640\n",
            "672\n",
            "704\n",
            "736\n",
            "768\n",
            "800\n",
            "832\n",
            "864\n",
            "896\n",
            "928\n",
            "960\n",
            "992\n",
            "1024\n",
            "1056\n",
            "1088\n",
            "1120\n",
            "1152\n",
            "1184\n",
            "1216\n",
            "1248\n",
            "1280\n",
            "1312\n",
            "1344\n",
            "1376\n",
            "1408\n",
            "1440\n",
            "1472\n",
            "1504\n",
            "1536\n",
            "1568\n",
            "1600\n",
            "1632\n",
            "1664\n",
            "1696\n",
            "1728\n",
            "1760\n",
            "1792\n",
            "1824\n",
            "1856\n",
            "1888\n",
            "1920\n",
            "1952\n",
            "1984\n",
            "2016\n",
            "2048\n",
            "2080\n",
            "2112\n",
            "2144\n",
            "2176\n",
            "2208\n",
            "2240\n",
            "2272\n",
            "2304\n",
            "2336\n",
            "2368\n",
            "2400\n",
            "2432\n",
            "2464\n",
            "2496\n",
            "2528\n",
            "2560\n",
            "2592\n",
            "2624\n",
            "2656\n",
            "2688\n",
            "2720\n",
            "2752\n",
            "2784\n",
            "2816\n",
            "2848\n",
            "2880\n",
            "2912\n",
            "2944\n",
            "2976\n",
            "3008\n",
            "3040\n",
            "3072\n",
            "3104\n",
            "3136\n",
            "3168\n",
            "3200\n",
            "3232\n",
            "3264\n",
            "3296\n",
            "3328\n",
            "3360\n",
            "3392\n",
            "3424\n",
            "3456\n",
            "3488\n",
            "3520\n",
            "3552\n",
            "3584\n",
            "3616\n",
            "3648\n",
            "3680\n",
            "3712\n",
            "3744\n",
            "3776\n",
            "3808\n",
            "3840\n",
            "3872\n",
            "3904\n",
            "3936\n",
            "3968\n",
            "4000\n",
            "4032\n",
            "4064\n",
            "4096\n",
            "4128\n",
            "4160\n",
            "4192\n",
            "4224\n",
            "4256\n",
            "4288\n",
            "4320\n",
            "4352\n",
            "4384\n",
            "4416\n",
            "4448\n",
            "4480\n",
            "4512\n",
            "4544\n",
            "4576\n",
            "4608\n",
            "4640\n",
            "4672\n",
            "4704\n",
            "4736\n",
            "4768\n",
            "4800\n",
            "4832\n",
            "4864\n",
            "4896\n",
            "4928\n",
            "4960\n",
            "4992\n",
            "5024\n",
            "5056\n",
            "5088\n",
            "5120\n",
            "5152\n",
            "5184\n",
            "5216\n",
            "5248\n",
            "5280\n",
            "5312\n",
            "5344\n",
            "5376\n",
            "5408\n",
            "5440\n",
            "5472\n",
            "5504\n",
            "5536\n",
            "5568\n",
            "5600\n",
            "5632\n",
            "5664\n",
            "5696\n",
            "5728\n",
            "5760\n",
            "5792\n",
            "5824\n",
            "5856\n",
            "5888\n",
            "5920\n",
            "5952\n",
            "5984\n",
            "6016\n",
            "6048\n",
            "6080\n",
            "6112\n",
            "6144\n",
            "6176\n",
            "6208\n",
            "6240\n",
            "6272\n",
            "6304\n",
            "6336\n",
            "6368\n",
            "6400\n",
            "6432\n",
            "6464\n",
            "6496\n",
            "6528\n",
            "6560\n",
            "6592\n",
            "6624\n",
            "6656\n",
            "6688\n",
            "6720\n",
            "6752\n",
            "6784\n",
            "6816\n",
            "6848\n",
            "6880\n",
            "6912\n",
            "6944\n",
            "6976\n",
            "7008\n",
            "7040\n",
            "7072\n",
            "7104\n",
            "7136\n",
            "7168\n",
            "7200\n",
            "7232\n",
            "7264\n",
            "7296\n",
            "7328\n",
            "7360\n",
            "7392\n",
            "7424\n",
            "7456\n",
            "7488\n",
            "7520\n",
            "7552\n",
            "7584\n",
            "7616\n",
            "7648\n",
            "7680\n",
            "7712\n",
            "7744\n",
            "7776\n",
            "7808\n",
            "7840\n",
            "7872\n",
            "7904\n",
            "7936\n",
            "7968\n",
            "8000\n",
            "8032\n",
            "8064\n",
            "8096\n",
            "8128\n",
            "8160\n",
            "8192\n",
            "8224\n",
            "8256\n",
            "8288\n",
            "8320\n",
            "8352\n",
            "8384\n",
            "8416\n",
            "8448\n",
            "8480\n",
            "8512\n",
            "8544\n",
            "8576\n",
            "8608\n",
            "8640\n",
            "8672\n",
            "8704\n",
            "8736\n",
            "8768\n",
            "8800\n",
            "8832\n",
            "8864\n",
            "8896\n",
            "8928\n",
            "8960\n",
            "8992\n",
            "9024\n",
            "9056\n",
            "9088\n",
            "9120\n",
            "9152\n",
            "9184\n",
            "9216\n",
            "9248\n",
            "9280\n",
            "9312\n",
            "9344\n",
            "9376\n",
            "9408\n",
            "9440\n",
            "9472\n",
            "9504\n",
            "9536\n",
            "9568\n",
            "9600\n",
            "9632\n",
            "9664\n",
            "9696\n",
            "9728\n",
            "9760\n",
            "9792\n",
            "9824\n",
            "9856\n",
            "9888\n",
            "9920\n",
            "9952\n",
            "9984\n",
            "10016\n",
            "10048\n",
            "10080\n",
            "10112\n",
            "10144\n",
            "10176\n",
            "10208\n",
            "10240\n",
            "10272\n",
            "10304\n",
            "10336\n",
            "10368\n",
            "10400\n",
            "10432\n",
            "10464\n",
            "10496\n",
            "10528\n",
            "10560\n",
            "10592\n",
            "10624\n",
            "10656\n",
            "10688\n",
            "10720\n",
            "10752\n",
            "10784\n",
            "10816\n",
            "10848\n",
            "10880\n",
            "10912\n",
            "10944\n",
            "10976\n",
            "11008\n",
            "11040\n",
            "11072\n",
            "11104\n",
            "11136\n",
            "11168\n",
            "11200\n",
            "11232\n",
            "11264\n",
            "11296\n",
            "11328\n",
            "11360\n",
            "11392\n",
            "11424\n",
            "11456\n",
            "11488\n",
            "11520\n",
            "11552\n",
            "11584\n",
            "11616\n",
            "11648\n",
            "11680\n",
            "11712\n",
            "11744\n",
            "11776\n",
            "11808\n",
            "11840\n",
            "11872\n",
            "11904\n",
            "11936\n",
            "11968\n",
            "12000\n",
            "12032\n",
            "12064\n",
            "12096\n",
            "12128\n",
            "12160\n",
            "12192\n",
            "12224\n",
            "12256\n",
            "12288\n",
            "12320\n",
            "12352\n",
            "12384\n",
            "12416\n",
            "12448\n",
            "12480\n",
            "12512\n",
            "12544\n",
            "12576\n",
            "12608\n",
            "12640\n",
            "12672\n",
            "12704\n",
            "12736\n",
            "12768\n",
            "12800\n",
            "12832\n",
            "12864\n",
            "12896\n",
            "12928\n",
            "12960\n",
            "12992\n",
            "13024\n",
            "13056\n",
            "13088\n",
            "13115\n",
            "32\n",
            "64\n",
            "96\n",
            "128\n",
            "160\n",
            "192\n",
            "224\n",
            "256\n",
            "288\n",
            "320\n",
            "352\n",
            "384\n",
            "416\n",
            "448\n",
            "480\n",
            "512\n",
            "544\n",
            "576\n",
            "608\n",
            "640\n",
            "672\n",
            "704\n",
            "736\n",
            "768\n",
            "800\n",
            "832\n",
            "864\n",
            "896\n",
            "928\n",
            "960\n",
            "992\n",
            "1024\n",
            "1056\n",
            "1088\n",
            "1120\n",
            "1152\n",
            "1184\n",
            "1216\n",
            "1248\n",
            "1280\n",
            "1312\n",
            "1344\n",
            "1376\n",
            "1408\n",
            "1440\n",
            "1472\n",
            "1504\n",
            "1536\n",
            "1568\n",
            "1600\n",
            "1632\n",
            "1664\n",
            "1696\n",
            "1728\n",
            "1760\n",
            "1792\n",
            "1824\n",
            "1856\n",
            "1888\n",
            "1920\n",
            "1952\n",
            "1984\n",
            "2016\n",
            "2048\n",
            "2080\n",
            "2112\n",
            "2144\n",
            "2176\n",
            "2208\n",
            "2240\n",
            "2272\n",
            "2304\n",
            "2336\n",
            "2368\n",
            "2400\n",
            "2432\n",
            "2464\n",
            "2496\n",
            "2528\n",
            "2560\n",
            "2592\n",
            "2624\n",
            "2656\n",
            "2688\n",
            "2720\n",
            "2752\n",
            "2784\n",
            "2816\n",
            "2848\n",
            "2880\n",
            "2912\n",
            "2944\n",
            "2976\n",
            "3008\n",
            "3040\n",
            "3072\n",
            "3104\n",
            "3136\n",
            "3168\n",
            "3200\n",
            "3232\n",
            "3264\n",
            "3296\n",
            "3328\n",
            "3360\n",
            "3368\n",
            "ft_ResNet50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6EzvMgM2MIx",
        "colab_type": "code",
        "outputId": "de9b64c1-d90b-47cd-fdbf-42017e3682a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import scipy.io\n",
        "import torch\n",
        "import numpy as np\n",
        "#import time\n",
        "import os\n",
        "\n",
        "#######################################################################\n",
        "# Evaluate\n",
        "def evaluate(qf,ql,qc,gf,gl,gc):\n",
        "    query = qf.view(-1,1)\n",
        "    # print(query.shape)\n",
        "    score = torch.mm(gf,query)\n",
        "    score = score.squeeze(1).cpu()\n",
        "    score = score.numpy()\n",
        "    # predict index\n",
        "    index = np.argsort(score)  #from small to large\n",
        "    index = index[::-1]\n",
        "    # index = index[0:2000]\n",
        "    # good index\n",
        "    query_index = np.argwhere(gl==ql)\n",
        "    camera_index = np.argwhere(gc==qc)\n",
        "\n",
        "    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n",
        "    junk_index1 = np.argwhere(gl==-1)\n",
        "    junk_index2 = np.intersect1d(query_index, camera_index)\n",
        "    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n",
        "    \n",
        "    CMC_tmp = compute_mAP(index, good_index, junk_index)\n",
        "    return CMC_tmp\n",
        "\n",
        "\n",
        "def compute_mAP(index, good_index, junk_index):\n",
        "    ap = 0\n",
        "    cmc = torch.IntTensor(len(index)).zero_()\n",
        "    if good_index.size==0:   # if empty\n",
        "        cmc[0] = -1\n",
        "        return ap,cmc\n",
        "\n",
        "    # remove junk_index\n",
        "    mask = np.in1d(index, junk_index, invert=True)\n",
        "    index = index[mask]\n",
        "\n",
        "    # find good_index index\n",
        "    ngood = len(good_index)\n",
        "    mask = np.in1d(index, good_index)\n",
        "    rows_good = np.argwhere(mask==True)\n",
        "    rows_good = rows_good.flatten()\n",
        "    \n",
        "    cmc[rows_good[0]:] = 1\n",
        "    for i in range(ngood):\n",
        "        d_recall = 1.0/ngood\n",
        "        precision = (i+1)*1.0/(rows_good[i]+1)\n",
        "        if rows_good[i]!=0:\n",
        "            old_precision = i*1.0/rows_good[i]\n",
        "        else:\n",
        "            old_precision=1.0\n",
        "        ap = ap + d_recall*(old_precision + precision)/2\n",
        "\n",
        "    return ap, cmc\n",
        "\n",
        "######################################################################\n",
        "result = scipy.io.loadmat('/content/drive/My Drive/Re-ID/Person_reID_baseline_pytorch/model/HPM_tripletcrossentropy_pytorch_result.mat')\n",
        "query_feature = torch.FloatTensor(result['query_f'])\n",
        "query_cam = result['query_cam'][0]\n",
        "query_label = result['query_label'][0]\n",
        "gallery_feature = torch.FloatTensor(result['gallery_f'])\n",
        "gallery_cam = result['gallery_cam'][0]\n",
        "gallery_label = result['gallery_label'][0]\n",
        "\n",
        "# multi = os.path.isfile('HPM_multi_query.mat')\n",
        "\n",
        "# if multi:\n",
        "#     m_result = scipy.io.loadmat('multi_query.mat')\n",
        "#     mquery_feature = torch.FloatTensor(m_result['mquery_f'])\n",
        "#     mquery_cam = m_result['mquery_cam'][0]\n",
        "#     mquery_label = m_result['mquery_label'][0]\n",
        "#     mquery_feature = mquery_feature.cuda()\n",
        "\n",
        "query_feature = query_feature.cuda()\n",
        "gallery_feature = gallery_feature.cuda()\n",
        "\n",
        "print(query_feature.shape)\n",
        "CMC = torch.IntTensor(len(gallery_label)).zero_()\n",
        "ap = 0.0\n",
        "#print(query_label)\n",
        "for i in range(len(query_label)):\n",
        "    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n",
        "    if CMC_tmp[0]==-1:\n",
        "        continue\n",
        "    CMC = CMC + CMC_tmp\n",
        "    ap += ap_tmp\n",
        "    #print(i, CMC_tmp[0])\n",
        "\n",
        "CMC = CMC.float()\n",
        "CMC = CMC/len(query_label) #average CMC\n",
        "print('Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n",
        "\n",
        "# multiple-query\n",
        "# CMC = torch.IntTensor(len(gallery_label)).zero_()\n",
        "# ap = 0.0\n",
        "# if multi:\n",
        "#     for i in range(len(query_label)):\n",
        "#         mquery_index1 = np.argwhere(mquery_label==query_label[i])\n",
        "#         mquery_index2 = np.argwhere(mquery_cam==query_cam[i])\n",
        "#         mquery_index =  np.intersect1d(mquery_index1, mquery_index2)\n",
        "#         mq = torch.mean(mquery_feature[mquery_index,:], dim=0)\n",
        "#         ap_tmp, CMC_tmp = evaluate(mq,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n",
        "#         if CMC_tmp[0]==-1:\n",
        "#             continue\n",
        "#         CMC = CMC + CMC_tmp\n",
        "#         ap += ap_tmp\n",
        "#         #print(i, CMC_tmp[0])\n",
        "#     CMC = CMC.float()\n",
        "#     CMC = CMC/len(query_label) #average CMC\n",
        "#     print('multi Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3368, 30720])\n",
            "Rank@1:0.922506 Rank@5:0.972090 Rank@10:0.985154 mAP:0.784705\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}